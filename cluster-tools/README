=============================================================================
Jahia Cluster Tools
=============================================================================

Introduction
-----------------------------------------------------------------------------
This small command line utility makes it easy to manage large cluster of
Jahia installations. It has an initial setup to go through which might seem
a little tedious at first but once it is done you will gain a LOT of time :)

Features
-----------------------------------------------------------------------------
- Generates the jahia.advanced.properties file for all the nodes in the
cluster based on a single cluster.properties configuration file. Makes it
easy to modify all the files at once
- Uses a template directory to copy any file to all cluster nodes
- Can run command on the cluster nodes such as start,stop,kill,ps or any
other UNIX command.
- Starts the Tomcat instances sequentially, waiting for each to finish
starting up before starting the next (which is the proper way to start a
cluster)
- Makes it easy to deploy a new module to the whole cluster
- Supports filtering of template files to insert the cluster identifier
in configuration or script files (like the YourKit startup_with_yjp.sh
startup script for example).
- Can tail the logs and aggregate them into a single terminal window
- Can retrieve the external host names and internal IPs of Amazon EC 2
instances.

Limitations
-----------------------------------------------------------------------------
- Assumes that all cluster nodes will deploy Jahia to the same directory
- Assumes that all cluster nodes are accessible through SSH using public
/private key pairs

Command help:
-----------------------------------------------------------------------------
To see the available options in the command line, use :

java -jar cluster-tools-VERSION-with-deps.jar

It is recommended to have a quick look at the possible options, as some of
them might be very useful to you.

Complete walkthrough
-----------------------------------------------------------------------------
In this section we will give a complete example of using the cluster tools
software to help configure a cluster quickly and efficiently.

We will be assuming that we have the following machines :

- 192.168.0.1 which will be configured as the processing server
- 192.168.0.2 which will be configured as a contribution server
- 192.168.0.3 which will be configured as a browsing server

We will also assume that the Jahia installation will be done in the
/home/user/jahia directory for the user named "user".

1. First we will need to setup an SSH public/private keypair to use with
the cluster-tools utility. If you do not know how to do this, here is a
quick example, to perform from the machine that you will use to run
the cluster-tools :

ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/user/.ssh/id_rsa):
Created directory '/home/user/.ssh'.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/user/.ssh/id_rsa.
Your public key has been saved in /home/user/.ssh/id_rsa.pub.
The key fingerprint is:
3e:4f:05:79:3a:9f:96:7c:3b:ad:e9:58:37:bc:37:e4 user@192.168.0.1

Add the id_rsa.pub contents to the /home/user/.ssh/authorizedkeys for
each machine

2. Install Jahia with the console mode installer on 192.168.0.1 During
the install, make sure you do NOT use Derby, but a proper shared
database, make sure you select database storage for the files and
content and activate the cluster configuration and enter only the IP
for 192.168.0.1 (although you could enter all of them but this is to
illustrate that you don't need to know all the IPs during the initial
installation phase).

3. Start the Jahia on 192.168.0.1 and perform any initial setup you want,
for example maybe importing a site and publishing it. Once you are done,
shutdown the Jahia server on the node properly (don't kill it).

4. Copy the cluster.example directory from the config-tools ZIP package
and its content into a new directory, for example mycluster. You will
want to create the new directory in the same directory as the cluster-tools*.jar
so it's easy to reference it on the command line.

5. In the mycluster/cluster.properties modify the nodeTypes value :

# General config
nodeTypes=processing,contribution,browsing

Basically you should have a nodeType for each of your nodes. These nodes
types will be used to identify the processing server, and also to generate
the node names. The possible node type values are : processing,
contribution or browsing. For example, let's imagine you have the following
configuration :

nodeTypes=processing,contribution,contribution,browsing,browsing,contribution,browsing

The cluster configuration will generate the following names :

  - processing1
  - contribution1
  - contribution2
  - browsing1
  - browsing2
  - contribution3
  - browsing3

Remember that only one processing server is allowed !

If you need to customize the prefixes generated, you can do so with the next
parameters in the cluster.properties file.

browsingNodeNamePrefix=browsing
contributionNodeNamePrefix=contribution
processingNodeNamePrefix=processing

Usually you will not need to change these.

6. Enter the host names and internal IPs (usually the same, but in some cases like
the case of Amazon EC2 deployment they might be different). Also remember that they
must match the number of node types you configured in step 5.

externalHostNames=192.168.0.1,192.168.0.2,192.168.0.3
internalIPs=192.168.0.1,192.168.0.2,192.168.0.3

7. Configure the SSH authentification section. Point it to the private key you generated,
as well as the user name you will be using to connect using SSH. Basically the SSH connection
to each node will be done like this SSH equivalent :

ssh -i ${privateKeyFileLocation} ${deploymentUserName}@${externalHostName}:${deploymentTargetPath}

# Auth config
privateKeyFileLocation=/home/user/.ssh/id_rsa
deploymentUserName=user
deploymentTargetPath=/home/user/jahia

8. In the staging configuration, you can keep the defaults, you will just want to adjust the remote
log directory to match the deployment target path.

# Staging config
jahiaAdvancedPropertyRelativeFileLocation=tomcat/webapps/ROOT/WEB-INF/etc/config/jahia.advanced.properties
templateDirectoryName=template
nodesDirectoryName=nodes
logsDirectory=logs
remoteLogDirectory=/home/user/jahia/tomcat/logs
filesToFilter=
deleteFileNamePrefix=CLUSTER_DELETE_ME_

9. In the commands config, again change the (/home/user/jahia) deployment target path to whatever you
use on your servers. You can leave the rest as it is.

# Commands config
waitForStartupURL=http://${hostname}:8080/welcome
startupCommandLine=/home/user/jahia/tomcat/bin/startup.sh
shutdownCommandLine=/home/user/jahia/tomcat/bin/shutdown.sh
getPidCommandLine=cat /home/user/jahia/tomcat/temp/tomcat.pid
psCommandLine=ps aux | grep bootstrap.jar
killCommandLine=kill ${tomcatpid}
hardKillCommandLine=kill -9 ${tomcatpid}
dumpThreadCommandLine=kill -QUIT ${tomcatpid}
tailLogsCommandLine=tail -f /home/user/jahia/tomcat/logs/catalina.out

10. If you don't use Amazon EC 2 for deploment, you can safely ignore this following section.

# Amazon Web Services config
awsAccessKey=
awsSecretKey=
awsInstanceNamesToIgnore=injector

11. The database configuration is used to synchronize the journal revision (that is to say the last
modification that the backend storage has seen), which will speed up first startup of the node. Here
to populate this the best thing is to copy the values from the first nodes tomcat/conf/Catalina/localhost/ROOT.xml
file. You do not need to change the dbLocalRevisionsTableName or dbGlobalRevisionTableName

# Database config
dbDriverClass=
dbUrl=
dbUser=jahia
dbPassword=jahia
dbLocalRevisionsTableName=JR_J_LOCAL_REVISIONS
dbGlobalRevisionTableName=JR_J_GLOBAL_REVISIONS

12. The integrity checks section defaults should be fine, simply change the login user name and password to match
the administrator's login information.

# Integrity checks configuration
loginURL=http://${hostname}:8080/cms/login
logoutURL=http://${hostname}:8080/cms/logout
loginUserName=root
loginPassword=root1234
cacheChecksumURL=http://${hostname}:8080/cms/cachetools/checksums
cacheKeyFlushURL=http://${hostname}:8080/cms/cachetools/flushkeys

13. On 192.168.0.1, compress the Jahia installation and copy it to the other
nodes. Don't worry about the processing server configuration, this will be
automatically modified by the cluster tools utility.

14. Now let's check that the cluster-tools configuration works by simply
launching :

java -jar cluster-tools-VERSION-with-deps.jar mycluster ps

If all goes well you should see the results of the command :
  ps aux | grep bootstrap.jar

Make sure you launch this from a directory above the mycluster directory.

for all nodes in the cluster

15. If all works properly, we can now start configuring the cluster. In
order to do this simply run :

java -jar cluster-tools-VERSION-with-deps.jar mycluster

This will do three things :
- copy the contents of the templates directory into nodes subdirectories,
one for each subnode
- configure the jahia.advanced.properties file for each node, in it's
respective directory
- deploy the configuration to each cluster node using SSH file copies

16. Now, before we start the nodes, we must update the database to
indicate that all the nodes should start at the same revision, otherwise
they will start replaying modifications since the dawn of time (=install)
which could take for ever. To do this simply launch :

java -jar cluster-tools-VERSION-with-deps.jar mycluster updatelocalrevisions

17. You can now launch the cluster using the command :

java -jar cluster-tools-VERSION-with-deps.jar mycluster start

This will launch each node sequentially, in the order specified in the
cluster.properties file. Once the utility completes, all nodes have
been started.

18. In a seperate terminal window, you can launch :

java -jar cluster-tools-VERSION-with-deps.jar mycluster taillogs

This will tail the logs of all the nodes simultaneously, prefixing each line
with the name of the node.

19. To stop the cluster, simply launch :

java -jar cluster-tools-VERSION-with-deps.jar mycluster stop

check the status with the following command :

java -jar cluster-tools-VERSION-with-deps.jar mycluster ps

If for some reasons the nodes do not properly shutdown, you can first try :

java -jar cluster-tools-VERSION-with-deps.jar mycluster kill

and if that still doesn't work you can use :

java -jar cluster-tools-VERSION-with-deps.jar mycluster hardkill

Beware : that last command will initiate a kill -9 on the Jahia process, so
it might lead to data loss and/or corruption.

Deploying a new module to a cluster
-----------------------------------------------------------------------------

In this section we assume that you have completed the above steps in the
complete walkthrough.

1. Copy your mycluster directory to a new directory we will call newapp

2. Copy the new module into the following directory :

newapp/templates/tomcat/webapps/ROOT/WEB-INF/var/shared_modules

3. Remove the jahia.advanced.properties file from the directory

newapp/templates/tomcat/webapps/ROOT/WEB-INF/etc/config

4. Launch :

java -jar cluster-tools-VERSION-with-deps.jar newapp

Using this technique, you can see that it is very easy to generate seperate
projects for deployment to the cluster.

If you have deployed a module that embeds libraries or taglibs, you will need
to restart your cluster before you can use it. Simply use the "stop" and
"start" commands from the cluster-tools utility.

Filtering files
-----------------------------------------------------------------------------

If you need to filter files to insert the node identifier into them, you can
do so by listing them in the cluster.properties file like in the following
example :

filesToFilter=tomcat/bin/startup_with_yjp.sh

You can specify multiple files simply by seperating them with a comma.

So if the startup_with_yjp.sh contains the following marker :

#{cluster.nodeId}

like in the following example :

JAVA_OPTS="-agentpath:/home/user/yjp-9.5.6/bin/linux-x86-64/libyjpagent.so=disablestacktelemetry,disableexceptiontelemetry,builtinprobes=none,delay=10000,sessionname=#{cluster.nodeId} $JAVA_OPTS"

this will become for example (for the 7th browsing node) :

JAVA_OPTS="-agentpath:/home/ec2-user/yjp-9.5.6/bin/linux-x86-64/libyjpagent.so=disablestacktelemetry,disableexceptiontelemetry,builtinprobes=none,delay=10000,sessionname=browsing7 $JAVA_OPTS"

For the moment only the #{cluster.nodeId} variable is supported, but we
might add more over time.

The filtering happens automatically when you launch the following command:

java -jar cluster-tools-VERSION-with-deps.jar mycluster

or if you prefer to check before deployment :

java -jar cluster-tools-VERSION-with-deps.jar mycluster configure

Using with Amazon Web Services
-----------------------------------------------------------------------------

In this section we do not cover full installation of Jahia on Amazon EC2,
we only specify the configuration specific to using the cluster-tools utility
with an existing cluster.

If you want to use the cluster-tools to configure and deploy to Amazon EC2
instances, make sure you have setup Jahia with for example an Amazon RDS
database, and make sure to configure the AWS secret key and access key in
 the cluster.properties files as illustrated below :

awsAccessKey=
awsSecretKey=
awsInstanceNamesToIgnore=injector

The awsInstanceNamesToIgnore is a prefix for an instance name that will be
ignore on all AWS specific operations.

Also, make sure you modify the cluster.properties configuration to use
the keypair private file for authentification and change the user name
to ec2-user or whatever the default is for your instances.

You can for example automatically retrieve all running instances in an AWS
cluster (using the default region, we don't support any other for the moment)
with the following command :

java -jar cluster-tools-VERSION-with-deps.jar mycluster awsgetinstances

This will automatically populate the externalHostNames and internalIP lists
for you based on the node types you provided. It will also ignore any
instance name with the prefixes specified (so anything starting with
"injector" in the above example).

Adding nodes dynamically
-----------------------------------------------------------------------------

Note that it IS possible to add cluster nodes dynamically, as the list of
IP addresses does not need to be updated on all the nodes, as in Jahia it
is only used to describe the initial members of the cluster, you may add
more at a later time. Removing nodes dynamically should also be possible,
but if you do not restart the whole cluster, if you remove a node that is
listed as an initial member you will get a lot of warnings in the logs.

